{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from fmri_decoder.data import DataConfig, ModelConfig, SurfaceData, TimeseriesData\n",
    "from fmri_decoder.model import MVPA\n",
    "from fmri_decoder.preprocessing import (\n",
    "    FeatureSelection,\n",
    "    TimeseriesPreproc,\n",
    "    TimeseriesSampling,\n",
    ")\n",
    "\n",
    "# arguments\n",
    "CONFIG_IN = \"\"  # file name of yaml configuration file\n",
    "DIR_OUT = \"\"  # path of output directory to which all resulting files are written\n",
    "\n",
    "# make output directory\n",
    "dir_out = Path(DIR_OUT)\n",
    "dir_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dir_sample = dir_out / \"sample\"\n",
    "dir_label = dir_out / \"label\"\n",
    "dir_model = dir_out / \"model\"\n",
    "\n",
    "# load data\n",
    "time_data = TimeseriesData.from_yaml(CONFIG_IN)\n",
    "surf_data = SurfaceData.from_yaml(CONFIG_IN)\n",
    "config_data = DataConfig.from_yaml(CONFIG_IN)\n",
    "config_model = ModelConfig.from_yaml(CONFIG_IN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.data import Data\n",
    "import functools\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "class Univariate:\n",
    "    \"\"\"Compute the univariate profile for different number of features.\"\"\"\n",
    "\n",
    "    def __init__(self, subj, sess, day):\n",
    "        self.subj = subj\n",
    "        self.sess = sess\n",
    "        self.day = day\n",
    "        self.data = Data(self.subj, f\"{self.sess}{SESSION[self.subj][self.sess][self.day]}\")\n",
    "        self.label, self.hemi = self.get_label\n",
    "        self.label_sorted, self.hemi_sorted = zip(*[self.sort_features(i) for i in range(N_LAYER)])\n",
    "\n",
    "    @property\n",
    "    @functools.lru_cache()\n",
    "    def get_label(self):\n",
    "        \"\"\"Get label and hemisphere.\"\"\"\n",
    "        surf_data = SurfaceData(self.data.file_layer, None, self.data.file_label)\n",
    "\n",
    "        label_left = surf_data.load_label_intersection(\"lh\")\n",
    "        label_right = surf_data.load_label_intersection(\"rh\")\n",
    "\n",
    "        hemi = np.zeros(len(label_left) + len(label_right))\n",
    "        hemi[len(label_left):] = 1\n",
    "        label = np.append(label_left, label_right)\n",
    "\n",
    "        return label, hemi\n",
    "\n",
    "    def sort_features(self, layer):\n",
    "        \"\"\"Sort label and hemi array based on features.\"\"\"\n",
    "        dtf = pd.read_parquet(self.data.get_sample_data(layer))\n",
    "\n",
    "        # choose subset of features\n",
    "        features = dtf.columns[2:]\n",
    "        \n",
    "        X = np.array(dtf.loc[:, features])\n",
    "        y = np.array(dtf.loc[:, \"label\"])\n",
    "\n",
    "        f_statistic = f_classif(X, y)[0]\n",
    "        index = np.arange(len(features))\n",
    "        index_sorted = np.array(\n",
    "                    [x for _, x in sorted(zip(f_statistic, index), reverse=True)]\n",
    "                )\n",
    "\n",
    "        label_sorted= self.label[index_sorted]\n",
    "        hemi_sorted = self.hemi[index_sorted]\n",
    "\n",
    "        return label_sorted, hemi_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features selection\n",
    "features_selected = {}\n",
    "if surf_data.file_localizer is not None:\n",
    "    features = FeatureSelection.from_yaml(CONFIG_IN)\n",
    "    features_selected = features.sort_features(config_model.radius, config_model.nmax)\n",
    "\n",
    "# timeseries preprocessing\n",
    "preproc = TimeseriesPreproc.from_yaml(CONFIG_IN)\n",
    "# detrend time series\n",
    "_ = preproc.detrend_timeseries(config_data.tr, config_data.cutoff_sec)\n",
    "# crop time series\n",
    "data_vol, events = preproc.crop_data(config_data.n_skip)\n",
    "\n",
    "# control condition: randomize labels\n",
    "if config_model.randomize_labels:\n",
    "    for i in events:\n",
    "        np.random.shuffle(i)\n",
    "\n",
    "# iterate over surfaces (layers)\n",
    "n_surf = len(surf_data.file_layer[\"lh\"])\n",
    "for i in range(n_surf):\n",
    "    data_sampled = {}\n",
    "    for hemi in [\"lh\", \"rh\"]:\n",
    "        vtx, fac = surf_data.load_layer(hemi, i)\n",
    "        sampler = TimeseriesSampling(vtx, fac, data_vol)\n",
    "        # sample time series\n",
    "        file_deformation = config_data.file_deformation\n",
    "        file_reference = time_data.file_series[0]\n",
    "        data_sampled[hemi] = sampler.sample_timeseries(file_deformation, file_reference)\n",
    "        # filter time series\n",
    "        if config_data.filter_size:\n",
    "            label = surf_data.load_label_intersection(hemi)\n",
    "            data_sampled[hemi] = sampler.filter_timeseries(\n",
    "                label, config_data.filter_size\n",
    "            )\n",
    "\n",
    "    if surf_data.file_localizer is not None:\n",
    "        mvpa = MVPA.from_selected_data(data_sampled, features_selected, events)\n",
    "    else:\n",
    "        for hemi in [\"lh\", \"rh\"]:\n",
    "            label = surf_data.load_label_intersection(hemi)\n",
    "            data_sampled[hemi] = [\n",
    "                data_sampled[hemi][x][label, :] for x in range(len(data_sampled[hemi]))\n",
    "            ]\n",
    "        mvpa = MVPA.from_data(\n",
    "            data_sampled, events, nmax=config_model.nmax, remove_nan=True\n",
    "        )\n",
    "\n",
    "    # model preparation and fitting\n",
    "    # scaling\n",
    "    if config_model.feature_scaling:\n",
    "        mvpa.scale_features(config_model.feature_scaling)\n",
    "    if config_model.sample_scaling:\n",
    "        mvpa.scale_samples(config_model.sample_scaling)\n",
    "    _ = mvpa.evaluate\n",
    "\n",
    "    # save results\n",
    "    mvpa.save_results(dir_out / \"accuracy.csv\", \"accuracy\")\n",
    "    mvpa.save_results(dir_out / \"sensitivity.csv\", \"sensitivity\")\n",
    "    mvpa.save_results(dir_out / \"specificity.csv\", \"specificity\")\n",
    "    mvpa.save_results(dir_out / \"f1.csv\", \"f1\")\n",
    "\n",
    "    # compute p-value by permutation sampling and save to disk\n",
    "    N_ITER = 1000\n",
    "    mvpa.save_stats(dir_out / \"pval_accuracy.csv\", N_ITER, \"accuracy\")\n",
    "    mvpa.save_stats(dir_out / \"pval_sensitivity.csv\", N_ITER, \"sensitivity\")\n",
    "    mvpa.save_stats(dir_out / \"pval_specificity.csv\", N_ITER, \"specificity\")\n",
    "    mvpa.save_stats(dir_out / \"pval_f1.csv\", N_ITER, \"f1\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
