{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import re\n",
    "from pathlib import Path\n",
    "from fmri_decoder.data import DataConfig, ModelConfig, SurfaceData, TimeseriesData\n",
    "from fmri_decoder.model import MVPA\n",
    "from fmri_decoder.preprocessing import (\n",
    "    TimeseriesPreproc,\n",
    "    TimeseriesSampling,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from sklearn.feature_selection import f_classif\n",
    "from src.config import N_LAYER, SESSION, DIR_BASE, N_RUN\n",
    "\n",
    "class Data:\n",
    "    \"\"\"File paths to data.\"\"\"\n",
    "\n",
    "    def __init__(self, subj, sequence, day, area):\n",
    "        self.subj = subj\n",
    "        self.sess = f\"{sequence}{SESSION[self.subj][sequence][day]}\"\n",
    "        self.day = day\n",
    "        self.area = area\n",
    "\n",
    "    @property\n",
    "    def surfaces(self):\n",
    "        \"\"\"File names of surface geometries.\"\"\"\n",
    "        file_ = {}\n",
    "        for hemi in [\"lh\", \"rh\"]:\n",
    "            file_[hemi] = [\n",
    "                str(\n",
    "                    Path(DIR_BASE)\n",
    "                    / self.subj\n",
    "                    / \"anatomy\"\n",
    "                    / \"layer\"\n",
    "                    / f\"{hemi}.layer_{i}\"\n",
    "                )\n",
    "                for i in range(N_LAYER)\n",
    "            ]\n",
    "        return file_\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"File names of labels.\"\"\"\n",
    "        file_ = {}\n",
    "        for hemi in [\"lh\", \"rh\"]:\n",
    "            file_[hemi] = [\n",
    "                str(\n",
    "                    Path(DIR_BASE)\n",
    "                    / self.subj\n",
    "                    / \"anatomy\"\n",
    "                    / \"label_benson\"\n",
    "                    / f\"{hemi}.{self.area}.label\"\n",
    "                ),\n",
    "                str(\n",
    "                    Path(DIR_BASE)\n",
    "                    / self.subj\n",
    "                    / \"anatomy\"\n",
    "                    / \"label\"\n",
    "                    / f\"{hemi}.fov.label\"\n",
    "                )\n",
    "            ]\n",
    "        return file_\n",
    "\n",
    "    def get_sample_data(self, layer):\n",
    "        \"\"\"Load sample data from MVPA analysis.\"\"\"\n",
    "        file = (\n",
    "            Path(DIR_BASE)\n",
    "            / \"paper\"\n",
    "            / \"v2.0\"\n",
    "            / \"decoding\"\n",
    "            / self.subj\n",
    "            / self.sess\n",
    "            / f\"{self.area}_bandpass_none\"\n",
    "            / \"sample\"\n",
    "            / f\"sample_data_{layer}.parquet\"\n",
    "        )\n",
    "        return file\n",
    "\n",
    "    @property\n",
    "    def timeseries(self):\n",
    "        \"\"\"File names of fmri time series.\"\"\"\n",
    "        if \"VASO\" in self.sess and \"uncorrected\" in self.sess:\n",
    "            sess_ = re.sub(\"_uncorrected\", \"\", self.sess)\n",
    "            file_ = [\n",
    "                str(\n",
    "                    Path(DIR_BASE)\n",
    "                    / self.subj\n",
    "                    / \"odc\"\n",
    "                    / sess_\n",
    "                    / f\"Run_{i+1}\"\n",
    "                    / \"ubold_upsampled.nii\"\n",
    "                )\n",
    "                for i in range(N_RUN)\n",
    "            ]\n",
    "        elif \"VASO\" in self.sess:\n",
    "            file_ = [\n",
    "                str(\n",
    "                    Path(DIR_BASE)\n",
    "                    / self.subj\n",
    "                    / \"odc\"\n",
    "                    / self.sess\n",
    "                    / f\"Run_{i+1}\"\n",
    "                    / \"uvaso_upsampled_corrected.nii\"\n",
    "                )\n",
    "                for i in range(N_RUN)\n",
    "            ]\n",
    "        else:\n",
    "            file_ = [\n",
    "                str(\n",
    "                    Path(DIR_BASE)\n",
    "                    / self.subj\n",
    "                    / \"odc\"\n",
    "                    / self.sess\n",
    "                    / f\"Run_{i+1}\"\n",
    "                    / \"udata.nii\"\n",
    "                )\n",
    "                for i in range(N_RUN)\n",
    "            ]\n",
    "        return file_\n",
    "\n",
    "    @property\n",
    "    def events(self):\n",
    "        \"\"\"File names of condition files.\"\"\"\n",
    "        sess_ = (\n",
    "            re.sub(\"_uncorrected\", \"\", self.sess)\n",
    "            if \"_uncorrected\" in self.sess\n",
    "            else self.sess\n",
    "        )\n",
    "        file_ = [\n",
    "            str(\n",
    "                Path(DIR_BASE)\n",
    "                / self.subj\n",
    "                / \"odc\"\n",
    "                / sess_\n",
    "                / f\"Run_{i+1}\"\n",
    "                / \"logfiles\"\n",
    "                / f\"{self.subj}_{sess_}_Run{i+1}_odc_Cond.mat\"\n",
    "            )\n",
    "            for i in range(N_RUN)\n",
    "        ]\n",
    "        return file_\n",
    "\n",
    "    @property\n",
    "    def deformation(self):\n",
    "        \"\"\"File name of coordinate mapping.\"\"\"\n",
    "        sess_ = (\n",
    "            re.sub(\"_uncorrected\", \"\", self.sess)\n",
    "            if \"_uncorrected\" in self.sess\n",
    "            else self.sess\n",
    "        )\n",
    "        file_ = str(\n",
    "            Path(DIR_BASE)\n",
    "            / self.subj\n",
    "            / \"deformation\"\n",
    "            / \"odc\"\n",
    "            / sess_\n",
    "            / \"source2target.nii.gz\"\n",
    "        )\n",
    "        return file_\n",
    "\n",
    "\n",
    "class Univariate:\n",
    "    \"\"\"Compute the univariate profile for different number of features.\"\"\"\n",
    "\n",
    "    def __init__(self, subj, sess, day, area):\n",
    "        self.subj = subj\n",
    "        self.sess = sess\n",
    "        self.day = day\n",
    "        self.area = area\n",
    "        self.data = Data(self.subj, self.sess, self.day, self.area)\n",
    "        self.label, self.hemi = self.get_label\n",
    "        self.label_sorted, self.hemi_sorted = zip(*[self.sort_features(i) for i in range(N_LAYER)])\n",
    "\n",
    "    @property\n",
    "    @functools.lru_cache()\n",
    "    def get_label(self):\n",
    "        \"\"\"Get label and hemisphere.\"\"\"\n",
    "        surf_data = SurfaceData(self.data.surfaces, None, self.data.labels)\n",
    "\n",
    "        label_left = surf_data.load_label_intersection(\"lh\")\n",
    "        label_right = surf_data.load_label_intersection(\"rh\")\n",
    "\n",
    "        hemi = np.zeros(len(label_left) + len(label_right))\n",
    "        hemi[len(label_left):] = 1\n",
    "        label = np.append(label_left, label_right)\n",
    "\n",
    "        return label, hemi\n",
    "\n",
    "    def sort_features(self, layer):\n",
    "        \"\"\"Sort label and hemi array based on features.\"\"\"\n",
    "        dtf = pd.read_parquet(self.data.get_sample_data(layer))\n",
    "\n",
    "        # choose subset of features\n",
    "        features = dtf.columns[2:]\n",
    "        \n",
    "        X = np.array(dtf.loc[:, features])\n",
    "        y = np.array(dtf.loc[:, \"label\"])\n",
    "\n",
    "        f_statistic = f_classif(X, y)[0]\n",
    "        index = np.arange(len(features))\n",
    "        index_sorted = np.array(\n",
    "                    [x for _, x in sorted(zip(f_statistic, index), reverse=True)]\n",
    "                )\n",
    "\n",
    "        label_sorted= self.label[index_sorted]\n",
    "        hemi_sorted = self.hemi[index_sorted]\n",
    "\n",
    "        return label_sorted, hemi_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "DIR_OUT = \"/data/pt_01880/zzz\"\n",
    "SUBJ = \"p1\"\n",
    "SEQ = \"GE_EPI\"\n",
    "DAY = 0\n",
    "AREA = \"v1\"  # v1, v2, v3, v2a, v2b, v3a or v3b\n",
    "\n",
    "\n",
    "data = Data(SUBJ, SEQ, DAY, AREA)\n",
    "config: dict[str, int | float | str | None] = {}\n",
    "config[\"TR\"] = 3\n",
    "config[\"n_skip\"] = 2\n",
    "config[\"cutoff_sec\"] = 270\n",
    "config[\"filter_size\"] = None\n",
    "config[\"nmax\"] = 200\n",
    "config[\"radius\"] = None\n",
    "config[\"feature_scaling\"] = \"standard\"\n",
    "config[\"sample_scaling\"] = None\n",
    "config[\"file_series\"] = data.timeseries\n",
    "config[\"file_events\"] = data.events\n",
    "config[\"file_layer\"] = data.surfaces\n",
    "config[\"file_deformation\"] = data.deformation\n",
    "config[\"file_localizer\"] = None\n",
    "config[\"file_label\"] = data.labels\n",
    "config[\"randomize_labels\"] = False\n",
    "\n",
    "# make output directory\n",
    "dir_out = Path(DIR_OUT)\n",
    "dir_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dir_sample = dir_out / \"sample\"\n",
    "dir_label = dir_out / \"label\"\n",
    "dir_model = dir_out / \"model\"\n",
    "\n",
    "# load data\n",
    "time_data = TimeseriesData.from_dict(config)\n",
    "surf_data = SurfaceData.from_dict(config)\n",
    "config_data = DataConfig.from_dict(config)\n",
    "config_model = ModelConfig.from_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid = 5\n",
    "univariate = Univariate(SUBJ, SEQ, DAY, AREA)\n",
    "features_selected = {\"hemi\": univariate.hemi_sorted[mid][:config_model.nmax], \"label\": univariate.label_sorted[mid][:config_model.nmax]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detrend timeseries ...\n",
      "Crop timeseries ...\n"
     ]
    }
   ],
   "source": [
    "# timeseries preprocessing\n",
    "preproc = TimeseriesPreproc.from_dict(config)\n",
    "# detrend time series\n",
    "_ = preproc.detrend_timeseries(config_data.tr, config_data.cutoff_sec)\n",
    "# crop time series\n",
    "data_vol, events = preproc.crop_data(config_data.n_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 66)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over surfaces (layers)\n",
    "n_surf = len(surf_data.file_layer[\"lh\"])\n",
    "for i in range(n_surf):\n",
    "    data_sampled = {}\n",
    "    for hemi in [\"lh\", \"rh\"]:\n",
    "        vtx, fac = surf_data.load_layer(hemi, i)\n",
    "        sampler = TimeseriesSampling(vtx, fac, data_vol)\n",
    "        # sample time series\n",
    "        file_deformation = config_data.file_deformation\n",
    "        file_reference = time_data.file_series[0]\n",
    "        data_sampled[hemi] = sampler.sample_timeseries(file_deformation, file_reference)\n",
    "\n",
    "    if surf_data.file_localizer is not None:\n",
    "        mvpa = MVPA.from_selected_data(data_sampled, features_selected, events)\n",
    "    else:\n",
    "        for hemi in [\"lh\", \"rh\"]:\n",
    "            label = surf_data.load_label_intersection(hemi)\n",
    "            data_sampled[hemi] = [\n",
    "                data_sampled[hemi][x][label, :] for x in range(len(data_sampled[hemi]))\n",
    "            ]\n",
    "        mvpa = MVPA.from_data(\n",
    "            data_sampled, events, nmax=config_model.nmax, remove_nan=True\n",
    "        )\n",
    "\n",
    "    # model preparation and fitting\n",
    "    # scaling\n",
    "    if config_model.feature_scaling:\n",
    "        mvpa.scale_features(config_model.feature_scaling)\n",
    "    if config_model.sample_scaling:\n",
    "        mvpa.scale_samples(config_model.sample_scaling)\n",
    "    _ = mvpa.evaluate\n",
    "\n",
    "    # save results\n",
    "    mvpa.save_results(dir_out / \"accuracy.csv\", \"accuracy\")\n",
    "    mvpa.save_results(dir_out / \"sensitivity.csv\", \"sensitivity\")\n",
    "    mvpa.save_results(dir_out / \"specificity.csv\", \"specificity\")\n",
    "    mvpa.save_results(dir_out / \"f1.csv\", \"f1\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
